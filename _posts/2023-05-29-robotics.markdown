---
layout: post
title:  "Robotics"
date:   2023-05-29 21:26:14 -0400
categories: jekyll update
---

## Table of Conents
1. [IEEE SoutheastCon Robotics Competition](#item-1)
    1. [Project Overview](#sub1-item-1)
    2. [My Contribution](#sub1-item-2)
    3. [Simulation](#sub1-item3)
    4. [Robot Life-Cycle](#sub1-item-4)
    5. [Competition Results](#sub2-item-5)
2. [Scara Robot Simulation](#item-2)
    1. [Python GUI for Manual Control](#sub2-item-1)
    2. [Clicked Point Path Generation](#sub2-item-2)
    3. [Before Clicked Point](#sub2-item-3)
    4. [After Clicked Point](#sub2-item-4)

<a id="item-1"></a>
## IEEE SoutheastCon Robotics Competition

<img src= "https://suqjuan.github.io/docs/assets/img/workingActionShot.jpg" />

<a id="sub1-item-1"></a>
### Project Overview

The SoutheastCon Competition entailed traveling to Orlando, Florida with a fully autonomous robot to compete with 29 other universities. In preperation for competition I and 13 others worked tirelessly for 2 semesters with a goal of designing & implementing a robot that is capable of distinguishing between 5 different types of objects, and navigating to specific locations around the gameboard after completing tasks. Through our efforts and sleepless night we were able to pull through and win 2nd place!

<a id="sub1-item-2"></a>
### My Contribution
One area in which I focused my attention was the Robotic Operating System (ROS), a widely used platform for robotic software development. Through my own efforts, I gained a deep understanding of ROS and was able develop the back-end automation for our robot which allowed for inter-process communication between our C++ & Python programs. My API handled command requests, processed them accordingly and sent out appropriate commands to our manipulators, such as our arm. The API also handled data requests from our sensors such that our algorithms can make the best informed decisions. This ultimately allowed us to integrate different sensors and hardware into our robot software architecture, providing a powerful tool for controlling and simulating its performance in software such as Gazebo just by changing one parameter in code. Through enabling the simulation of the robot, we were able to test different strategies and algorithms, until performance was satisfactory and ready to run on the robot.

### Simulations
Talk about developing URDF in XML

Part of running simulations is having a robot model to visualize. This requires an understanding of URDF and writing code in XML. XML is a skill that I did not come into this project having, therefore while learning how to make a custom model I was able to import a model from this [reference repo] and begin testing algorithms.

<img src= "https://suqjuan.github.io/docs/assets/img/gazeboSim.jpg"/>

###### Nexus Robot Model Spawned in Custom Gameboard Environment




<a id="sub1-item-3"></a>
### Competition Results



1. Insert link for project github
2. Brief description of project
2. Picture and breif description of team
3. What was my contribution
4. Competition

***

<a id="item-2"></a>
## Scara Robot Simulation
Purpose of this project was to apply the principles of forward and inverse kinematics to a 4 link Scara Robot. The simulation was implemented using Robotic Operating System (ROS) and interfaced in python using the PyQT5 library for the GUI and  RVIZ/Gazebo for the 3D simulation. The simulation allows the user to manually control the robot using the GUI or have the robot follow a path generated by the user clicking on the RVIZ display. The forward kinematics were implemented using the Denavit-Hartenberg convention and the inverse kinematics were implemented using the geometric method. The simulation was developed in a Linux environment and the ROS package I created that includes all the  code can be cloned from [my repo] in the projects/scara directory on my GitHub.

<a id="sub2-item-1"></a>
### Python GUI for Manual Control
The followng GUI is prompted after running the custom launch file created for this part of the project:

```$ roslaunch robotics scara_gui.launch```

<img src= "https://suqjuan.github.io/docs/assets/img/scaraGUI.png"/>

The GUI enables the user to manually control each of the links individually using the sliders. The sliders are connected to ROS topics that publish the slider values to the ROS environment. The ROS nodes that are subscribed to these topics are the nodes that control the joint angles of the robot which are specified in the custom URDF file created for the robot.

<a id="sub2-item-2"></a>
### Clicked Point Path Generation
While the GUI I created is a great way to test the understanding of robotics principles, I wanted to take it a step further and implement an algorith which would not require the user to manually input values for each of the joints. Instead something more practical would be to implement an algorithm that enforces the inverse kinematics of the robot and calculates and updates joint parameters automatically. So this is exactly what I did. I started with a python program where a user can input a desired x,y,z cooridinate relative to the base frame in the program, but soon realized that this is not practical. Instead I took advantage of the ability to click on points in RVIZ, and publish the coordinates through a ROS message to my Python program instead of having to edit, run, edit, run each time I wanted to visualize a different end effector position. Thus, implemented the support for clicked points allowed for a continuous visual and form of validation of the inverse kinematics for the robot.

The followng program will run after executing the following commands:

First the Python program that handles all calculation and published updated joint parameters must be run using the following command:

``` $ rosrun robotics scara_FK.py```

Then the the custom launch file must be ran using:

``` $ roslaunch robotics scara.launch```

<a id="sub2-item-3"></a>
### Before Clicked Point 
<img src= "https://suqjuan.github.io/docs/assets/img/scaraBefore.png" />

The image above represents the initial position of the robot; the end-effector is the bottom tip of of the red link on the left.

<a id="sub2-item-4"></a>
### After Clicked Point
<img src= "https://suqjuan.github.io/docs/assets/img/scaraAfter.png"/>

Clicked points in RVIZ represented by pink spheres. In the above visualy we can see that after clicking on this position represented my the pink sphere, the inverse kinematics were executed properly since the end-effector was able to update it joint parameters appropriately to move the end-effector to the desired location.

***
[group repo]:https://github.com/ronniecm/ieee-robotics-2023-code
[reference repo]:https://github.com/RBinsonB/nexus_4wd_mecanum_simulator
[my repo]:https://github.com/Suqjuan/robotics




